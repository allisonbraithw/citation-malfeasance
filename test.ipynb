{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80127941-2f04-4b8f-a51b-e391524ce128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "parser = LlamaParse(\n",
    "    api_key=\"llx-0ZgX0YwVNEIqdL0qMwO0MeIHYro2m3j1g1HYKlnQxcMeREQb\",  # can also be set in your env as LLAMA_CLOUD_API_KEY\n",
    "    result_type=\"markdown\",  # \"markdown\" and \"text\" are available\n",
    "    num_workers=4,  # if multiple files passed, split in `num_workers` API calls\n",
    "    verbose=True,\n",
    "    language=\"en\",  # Optionally you can define a language, default=en\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80c62f3a-3dab-4a3f-a2b2-4a1635ba6404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id a92644eb-a208-4a8e-b095-4f7cc522d011\n"
     ]
    }
   ],
   "source": [
    "# sync\n",
    "fname = \"/Users/sourabhmadur/Downloads/fraud_version.pdf\"\n",
    "documents = parser.load_data(\"/Users/sourabhmadur/Downloads/fraud_version.pdf\")\n",
    "\n",
    "# # sync batch\n",
    "# documents = parser.load_data([\"./my_file1.pdf\", \"./my_file2.pdf\"])\n",
    "\n",
    "# # async\n",
    "# documents = await parser.aload_data(\"./my_file.pdf\")\n",
    "\n",
    "# # async batch\n",
    "# documents = await parser.aload_data([\"./my_file1.pdf\", \"./my_file2.pdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "719bdfae-8f2a-46b4-bc0e-4d7a539e003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9dc2149-56e5-498b-a5e7-99ac51a50e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ----fraud_version----- \n",
      "Pretraining Strategies for Structure Agnostic\n",
      "Material Property Prediction\n",
      "Hongshuo Huang,†,¶Rishikesh Magar,‡,¶and Amir Barati Farimani∗,‡,†\n",
      "†Department of Material Science and Engineering, Carnegie Mellon University, Pittsburgh\n",
      "PA, USA 15213\n",
      "‡Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh PA, USA\n",
      "15213\n",
      "¶Joint First Authorship\n",
      "E-mail: barati@cmu.edu\n",
      "Abstract\n",
      "In recent years, machine learning (ML), especially graph neural network (GNN)\n",
      "models, have been successfully used for fast and accurate prediction of material prop-\n",
      "erties. However, most ML models rely on relaxed crystal structures to develop de-\n",
      "scriptors for accurate predictions. Generating these relaxed crystal structures can be\n",
      "expensive and time-consuming, thus requiring an additional processing step for models\n",
      "that rely on them. To address this challenge, structure-agnostic methods have been\n",
      "developed, which use fixed-length descriptors engineered based on human knowledge\n",
      "about the material. However, the fixed-length descriptors are often hand-engineered\n",
      "and require extensive domain knowledge, and generally are not used in the context of\n",
      "learnable models which are known to have a superior performance. Recent advance-\n",
      "ments have proposed learnable frameworks that can construct representations based\n",
      "on stoichiometry alone, allowing the flexibility of using deep learning frameworks as\n",
      "well as leveraging structure-agnostic learning. In this work, we propose three different\n",
      "1pretraining strategies that can be used to pretrain these structure-agnostic learnable\n",
      "frameworks to further improve the downstream material property prediction perfor-\n",
      "mance. We incorporate strategies such as self-supervised learning (SSL), fingerprint\n",
      "learning (FL), and multimodal learning (ML) and demonstrate their efficacy on down-\n",
      "stream tasks for the Roost architecture, a popular structure-agnostic framework. Our\n",
      "results show significant improvement in small datasets and data efficiency in the larger\n",
      "datasets, underscoring the potential of our pretrain strategies that that effectively\n",
      "leverage unlabeled data for accurate material property prediction.\n",
      "2Introduction\n",
      "Machine learning (ML) models have made significant progress in computational material\n",
      "science, both in material property prediction1–10and new methods for material property\n",
      "prediction done by Xie et al.11The growth of ML models in material science has been fueled\n",
      "by the increasing number of publicly available datasets and improved hardware capabili-\n",
      "ties.12–14Popular ML frameworks take the crystalline structure as input and leverage Graph\n",
      "Neural Networks (GNNs) to construct representations that can be used for property predic-\n",
      "tion. In these frameworks, the crystal structure information like 3D coordinates is required\n",
      "to construct a graph of the crystalline material.9,15–18The general idea is to consider the\n",
      "atoms as the nodes and capture the interactions between them using edges. This structure\n",
      "captures the interactions in the crystalline material, and the models often take optimized\n",
      "structures that are generated via simulations or experiments. Despite the large availability\n",
      "of crystal structure in public repositories such as Materials Project19and ICSD,19it only\n",
      "represents a fraction of the chemical space of materials. Generating the crystalline struc-\n",
      "tures for all materials in the vast materials space can be a time-consuming process. This has\n",
      "motivated researchers to develop methods that do not require the structure of the material,\n",
      "for crystals that do not have a well defined structure beforehand. These structure agnos-\n",
      "tic methods can possibly be used for high throughput screening of materials with desired\n",
      "properties. The general approach to develop these structure agnostic models is using fixed\n",
      "length descriptors that encode the chemical composition of the material. These fixed length\n",
      "descriptors can be used to construct a feature vector that captures the material’s properties,\n",
      "which can be used to predict its behavior20–22˙However, the drawback of this approach is these\n",
      "fixed length descriptors need to be handcrafted and require considerable domain knowledge\n",
      "and expertise. Recently multiple approaches leveraging structure agnostic representations\n",
      "for material property predictions have been developed.17,23–26In this work, we focus on the\n",
      "Representation Learning from Stoichiometry (Roost) framework proposed by Goodall et al.1\n",
      "The Roost model takes as input the crystal formula and constructs an graph based repre-\n",
      "3sentation to develop a learnable framework. The Roost architecture is able to predict the\n",
      "material properties with a reasonable accuracy using only the stoichiometric data. In this\n",
      "work, we utilize the Roost model and propose three different pretraining strategies to improve\n",
      "the performance of the framework. Our pretraining strategies include 1.) Self Supervised\n",
      "Learning(SSL), 2.) Fingerprint Learning(FL) and 3.) Multimodal Learning(MML). After\n",
      "pretraining the Roost model with the 3 pretraining strategies we observe performance gains\n",
      "in multiple material property prediction tasks(Figure 1). The new generative model based\n",
      "on diffusion27is called CDVAE.\n",
      "For our first strategy, we propose the Self-Supervised Learning approach(SSL) for pretrain-\n",
      "ing the Roost encoder. In recent years, SSL frameworks28–36have been successfully utilized\n",
      "in computer vision and natural language processing tasks. The successful application of SSL\n",
      "has spurred many works in molecular machine learning37–39and material science.3,40Draw-\n",
      "ing upon the successful strategies of SSL employed in structure-based material property\n",
      "prediction,3,7we propose a framework for structure-agnostic SSL using the Roost encoder\n",
      "for generating material representation. The core idea of SSL revolves around pretraining\n",
      "models without the reliance on explicitly labeled datasets by leveraging the intrinsic infor-\n",
      "mation present in unlabeled data as the training signal. This framework can especially be\n",
      "advantageous for structure-agnostic material property prediction tasks, where labeled data\n",
      "may be scarce, and complete structural characterization of material is sometimes unavailable.\n",
      "By adopting this approach, we address the challenges associated with limited labeled data\n",
      "and inaccessible full structural information. For FL strategy, we devise a simple method-\n",
      "ology of predicting the Magpie fingerprint20using the Roost encoder, the core idea is that\n",
      "the pretrained model can learn the information captured by the fingerprint. Using such a\n",
      "strategy allows us build a Roost encoder that can retain the benefits of being a learnable\n",
      "framework and also capture information of a fixed descriptor like Magpie fingerprint. We\n",
      "also introduce a MML strategy in which we leverage the available characterized structure\n",
      "data and predict the embedding generated using a pretrained CGCNN1encoder from Crys-\n",
      "4tal Twins Framework.3Using such a strategy we are able to learn the structural information\n",
      "using our structure agnostic encoder. By incorporating these three strategies, we successfully\n",
      "enhance the performance of the Roost encoder on downstream tasks within the Matbench\n",
      "suite. Notably, we demonstrate improvements in most material property prediction tasks,\n",
      "highlighting the effectiveness and potential of our proposed pretraining strategies.\n",
      "Large Unlabeled DatasetFinetuningRoostEncoderMaterial Property PredictionSrTiO3Small Labeled DatasetPretraining StrategiesFingerprint\n",
      "Multimodal \n",
      "Self-Supervised SrTiO3SrTiO3xSrTiO3\n",
      "SrTiO3\n",
      "Figure 1: The framework for all the proposed pretraining strategies. We use the Roost\n",
      "encoder to demonstrate the effectiveness of the pretraining strategies for material property\n",
      "prediction tasks. We propose three strategies 1.) Self-Supervised Learning 2.) Fingerprint\n",
      "Learning and 3.) Multimodal Learning. Using these strategies we pretrain the Roost Encoder\n",
      "and finetune the model on different datasets in the Matbench41suite. Using such pretraining\n",
      "strategies we are able to demonstrate improvements on downstream tasks.\n",
      "References\n",
      "(1) Xie, T.; Grossman, J. C. Crystal graph convolutional neural networks for an accurate\n",
      "and interpretable prediction of material properties. Physical review letters 2018 ,120,\n",
      "145301.\n",
      "(2) Karamad, M.; Magar, R.; Shi, Y.; Siahrostami, S.; Gates, I. D.; Farimani, A. B. Orbital\n",
      "5graph convolutional neural network for material property prediction. Physical Review\n",
      "Materials 2020 ,4, 093801.\n",
      "(3) Magar, R.; Wang, Y.; Farimani, A. Crystal twins: self-supervised learning for crystalline\n",
      "material property prediction. npj Comput. Mater 2022 ,8, 231.\n",
      "(4) Choudhary, K.; DeCost, B. Atomistic Line Graph Neural Network for improved mate-\n",
      "rials property predictions. npj Computational Materials 2021 ,7, 1–8.\n",
      "(5) Louis, S.-Y.; Zhao, Y.; Nasiri, A.; Wang, X.; Song, Y.; Liu, F.; Hu, J. Graph convolu-\n",
      "tional neural networks with global attention for improved materials property prediction.\n",
      "Physical Chemistry Chemical Physics 2020 ,22, 18141–18148.\n",
      "(6) Chen, C.; Ye, W.; Zuo, Y.; Zheng, C.; Ong, S. P. Graph networks as a universal\n",
      "machine learning framework for molecules and crystals. Chemistry of Materials 2019 ,\n",
      "31, 3564–3572.\n",
      "(7) Cao, Z.; Magar, R.; Wang, Y.; Farimani, A. B. MOFormer: Self-Supervised Trans-\n",
      "former model for Metal-Organic Framework Property Prediction. arXiv preprint\n",
      "arXiv:2210.14188 2022 ,\n",
      "(8) Sch¨ utt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko, A.; M¨ uller, K.-R.\n",
      "SchNet–A deep learning architecture for molecules and materials. The Journal of Chem-\n",
      "ical Physics 2018 ,148, 241722.\n",
      "(9) Gasteiger, J.; Groß, J.; G¨ unnemann, S. Directional message passing for molecular\n",
      "graphs. arXiv preprint arXiv:2003.03123 2020 ,\n",
      "(10) Magar, R.; Farimani, A. B. Learning from mistakes: Sampling strategies to efficiently\n",
      "train machine learning models for material property prediction. Computational Mate-\n",
      "rials Science 2023 ,224, 112167.\n",
      "6(11) Xie, T.; Fu, X.; Ganea, O.-E.; Barzilay, R.; Jaakkola, T. S. Crystal Diffusion Variational\n",
      "Autoencoder for Periodic Material Generation. International Conference on Learning\n",
      "Representations. 2021.\n",
      "(12) Chen, A.; Zhang, X.; Zhou, Z. Machine learning: accelerating materials development\n",
      "for energy storage and conversion. InfoMat 2020 ,2, 553–576.\n",
      "(13) Schmidt, J.; Marques, M. R.; Botti, S.; Marques, M. A. Recent advances and applica-\n",
      "tions of machine learning in solid-state materials science. npj Computational Materials\n",
      "2019 ,5, 1–36.\n",
      "(14) Choudhary, K.; DeCost, B.; Chen, C.; Jain, A.; Tavazza, F.; Cohn, R.; Park, C. W.;\n",
      "Choudhary, A.; Agrawal, A.; Billinge, S. J.; Holm, E.; Ong, S. P.; Wolverton, C. Recent\n",
      "advances and applications of deep learning methods in materials science. npj Compu-\n",
      "tational Materials 2022 ,8, 59.\n",
      "(15) Park, C. W.; Wolverton, C. Developing an improved crystal graph convolutional neu-\n",
      "ral network framework for accelerated materials discovery. Physical Review Materials\n",
      "2020 ,4, 063801.\n",
      "(16) Yan, K.; Liu, Y.; Lin, Y.; Ji, S. Periodic Graph Transformers for Crystal Material\n",
      "Property Prediction. arXiv preprint arXiv:2209.11807 2022 ,\n",
      "(17) Ihalage, A.; Hao, Y. Formula Graph Self-Attention Network for Representation-Domain\n",
      "Independent Materials Discovery. Advanced Science 2022 , 2200164.\n",
      "(18) Gasteiger, J.; Giri, S.; Margraf, J. T.; G¨ unnemann, S. Fast and uncertainty-aware\n",
      "directional message passing for non-equilibrium molecules. 2020 ,\n",
      "(19) Belsky, A.; Hellenbrandt, M.; Karen, V. L.; Luksch, P. New developments in the Inor-\n",
      "ganic Crystal Structure Database (ICSD): accessibility in support of materials research\n",
      "and design. Acta Crystallographica Section B: Structural Science 2002 ,58, 364–369.\n",
      "7(20) Ward, L.; Agrawal, A.; Choudhary, A.; Wolverton, C. A general-purpose machine learn-\n",
      "ing framework for predicting properties of inorganic materials. npj Computational Ma-\n",
      "terials 2016 ,2, 16028.\n",
      "(21) Botu, V.; Batra, R.; Chapman, J.; Ramprasad, R. Machine learning force fields: con-\n",
      "struction, validation, and outlook. The Journal of Physical Chemistry C 2017 ,121,\n",
      "511–522.\n",
      "(22) Bart´ ok, A. P.; Kondor, R.; Cs´ anyi, G. On representing chemical environments. Physical\n",
      "Review B 2013 ,87, 184115.\n",
      "(23) Wang, A. Y.-T.; Kauwe, S. K.; Murdock, R. J.; Sparks, T. D. Compositionally restricted\n",
      "attention-based network for materials property predictions. Npj Computational Mate-\n",
      "rials2021 ,7, 77.\n",
      "(24) Chen, C.; Ong, S. P. AtomSets as a hierarchical transfer learning framework for small\n",
      "and large materials datasets. npj Computational Materials 2021 ,7, 173.\n",
      "(25) Goodall, R. E.; Lee, A. A. Predicting materials properties without crystal structure:\n",
      "Deep representation learning from stoichiometry. Nature Communications 2020 ,11,\n",
      "1–9.\n",
      "(26) Goodall, R. E.; Parackal, A. S.; Faber, F. A.; Armiento, R.; Lee, A. A. Rapid discov-\n",
      "ery of stable materials by coordinate-free coarse graining. Science Advances 2022 ,8,\n",
      "eabn4117.\n",
      "(27) Weng, L. What are diffusion models? lilianweng.github.io 2021 ,\n",
      "(28) Chen, T.; Kornblith, S.; Norouzi, M.; Hinton, G. A simple framework for contrastive\n",
      "learning of visual representations. International conference on machine learning. 2020;\n",
      "pp 1597–1607.\n",
      "8(29) Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; Joulin, A. Unsuper-\n",
      "vised learning of visual features by contrasting cluster assignments. arXiv preprint\n",
      "arXiv:2006.09882 2020 ,\n",
      "(30) Zbontar, J.; Jing, L.; Misra, I.; LeCun, Y.; Deny, S. Barlow twins: Self-supervised learn-\n",
      "ing via redundancy reduction. International Conference on Machine Learning. 2021; pp\n",
      "12310–12320.\n",
      "(31) He, K.; Fan, H.; Wu, Y.; Xie, S.; Girshick, R. Momentum contrast for unsupervised\n",
      "visual representation learning. Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition. 2020; pp 9729–9738.\n",
      "(32) Grill, J.-B.; Strub, F.; Altch´ e, F.; Tallec, C.; Richemond, P. H.; Buchatskaya, E.; Do-\n",
      "ersch, C.; Pires, B. A.; Guo, Z. D.; Azar, M. G.; Piot, B.; Kavukcuoglu, K.; Munos, R.;\n",
      "Valko, M. Bootstrap your own latent: A new approach to self-supervised learning. arXiv\n",
      "preprint arXiv:2006.07733 2020 ,\n",
      "(33) Chen, X.; He, K. Exploring simple siamese representation learning. Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021; pp 15750–\n",
      "15758.\n",
      "(34) Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. ALBERT: A\n",
      "Lite BERT for Self-supervised Learning of Language Representations. International\n",
      "Conference on Learning Representations. 2019.\n",
      "(35) Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidi-\n",
      "rectional transformers for language understanding. arXiv preprint arXiv:1810.04805\n",
      "2018 ,\n",
      "(36) Wu, J.; Wang, X.; Wang, W. Y. Self-Supervised Dialogue Learning. Proceedings of\n",
      "the 57th Annual Meeting of the Association for Computational Linguistics. 2019; pp\n",
      "3857–3867.\n",
      "9(37) Wang, Y.; Magar, R.; Liang, C.; Farimani, A. B. Improving Molecular Contrastive\n",
      "Learning via Faulty Negative Mitigation and Decomposed Fragment Contrast. arXiv\n",
      "preprint arXiv:2202.09346 2022 ,\n",
      "(38) Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V.; Leskovec, J. Strate-\n",
      "gies For Pre-training Graph Neural Networks. International Conference on Learning\n",
      "Representations (ICLR). 2020.\n",
      "(39) Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale\n",
      "Self-Supervised Pretraining for Molecular Property Prediction. arXiv preprint\n",
      "arXiv:2010.09885 2020 ,\n",
      "(40) Suzuki, Y.; Taniai, T.; Saito, K.; Ushiku, Y.; Ono, K. Self-supervised learning of ma-\n",
      "terials concepts from crystal structures via deep neural networks. Machine Learning:\n",
      "Science and Technology 2022 ,3, 045034.\n",
      "(41) Dunn, A.; Wang, Q.; Ganose, A.; Dopp, D.; Jain, A. Benchmarking materials property\n",
      "prediction methods: the Matbench test set and Automatminer reference algorithm. npj\n",
      "Computational Materials 2020 ,6, 1–10.\n",
      "10TOC Graphic\n",
      "11\n",
      "----End of fraud_version----- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Define the path to your folder containing the PDFs\n",
    "folder_path = 'papers/'\n",
    "\n",
    "# Prepare a long string to store all the extracted texts\n",
    "all_texts = \"\"\n",
    "\n",
    "# Iterate over each file in the specified folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Attempt to open and read the PDF file\n",
    "        try:\n",
    "            reader = PdfReader(file_path)\n",
    "            text = []\n",
    "            \n",
    "            # Read each page of the PDF\n",
    "            for page in reader.pages:\n",
    "                text.append(page.extract_text())\n",
    "            \n",
    "            # Join all pages' text and add it to the main string with formatting\n",
    "            all_texts += f\" ----{filename[:-4]}----- \\n{''.join(text)}\\n----End of {filename[:-4]}----- \\n\"\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {filename}: {str(e)}\")\n",
    "\n",
    "# Optional: print the complete text to check\n",
    "print(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "813a15af-d747-4c36-9496-a5a76f45532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = extract_pdf_pages(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa9a3200-c9ed-4219-add1-ae4a951e4b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--- START OF PDF $/Users/sourabhmadur/Downloads/fraud_version.pdf ---',\n",
       " '--- PAGE 0 ---',\n",
       " 'Pretraining Strategies for Structure Agnostic\\nMaterial Property Prediction\\nHongshuo Huang,†,¶Rishikesh Magar,‡,¶and Amir Barati Farimani∗,‡,†\\n†Department of Material Science and Engineering, Carnegie Mellon University, Pittsburgh\\nPA, USA 15213\\n‡Department of Mechanical Engineering, Carnegie Mellon University, Pittsburgh PA, USA\\n15213\\n¶Joint First Authorship\\nE-mail: barati@cmu.edu\\nAbstract\\nIn recent years, machine learning (ML), especially graph neural network (GNN)\\nmodels, have been successfully used for fast and accurate prediction of material prop-\\nerties. However, most ML models rely on relaxed crystal structures to develop de-\\nscriptors for accurate predictions. Generating these relaxed crystal structures can be\\nexpensive and time-consuming, thus requiring an additional processing step for models\\nthat rely on them. To address this challenge, structure-agnostic methods have been\\ndeveloped, which use fixed-length descriptors engineered based on human knowledge\\nabout the material. However, the fixed-length descriptors are often hand-engineered\\nand require extensive domain knowledge, and generally are not used in the context of\\nlearnable models which are known to have a superior performance. Recent advance-\\nments have proposed learnable frameworks that can construct representations based\\non stoichiometry alone, allowing the flexibility of using deep learning frameworks as\\nwell as leveraging structure-agnostic learning. In this work, we propose three different\\n1',\n",
       " '--- PAGE 1 ---',\n",
       " 'pretraining strategies that can be used to pretrain these structure-agnostic learnable\\nframeworks to further improve the downstream material property prediction perfor-\\nmance. We incorporate strategies such as self-supervised learning (SSL), fingerprint\\nlearning (FL), and multimodal learning (ML) and demonstrate their efficacy on down-\\nstream tasks for the Roost architecture, a popular structure-agnostic framework. Our\\nresults show significant improvement in small datasets and data efficiency in the larger\\ndatasets, underscoring the potential of our pretrain strategies that that effectively\\nleverage unlabeled data for accurate material property prediction.\\n2',\n",
       " '--- PAGE 2 ---',\n",
       " 'Introduction\\nMachine learning (ML) models have made significant progress in computational material\\nscience, both in material property prediction1–10and new methods for material property\\nprediction done by Xie et al.11The growth of ML models in material science has been fueled\\nby the increasing number of publicly available datasets and improved hardware capabili-\\nties.12–14Popular ML frameworks take the crystalline structure as input and leverage Graph\\nNeural Networks (GNNs) to construct representations that can be used for property predic-\\ntion. In these frameworks, the crystal structure information like 3D coordinates is required\\nto construct a graph of the crystalline material.9,15–18The general idea is to consider the\\natoms as the nodes and capture the interactions between them using edges. This structure\\ncaptures the interactions in the crystalline material, and the models often take optimized\\nstructures that are generated via simulations or experiments. Despite the large availability\\nof crystal structure in public repositories such as Materials Project19and ICSD,19it only\\nrepresents a fraction of the chemical space of materials. Generating the crystalline struc-\\ntures for all materials in the vast materials space can be a time-consuming process. This has\\nmotivated researchers to develop methods that do not require the structure of the material,\\nfor crystals that do not have a well defined structure beforehand. These structure agnos-\\ntic methods can possibly be used for high throughput screening of materials with desired\\nproperties. The general approach to develop these structure agnostic models is using fixed\\nlength descriptors that encode the chemical composition of the material. These fixed length\\ndescriptors can be used to construct a feature vector that captures the material’s properties,\\nwhich can be used to predict its behavior20–22˙However, the drawback of this approach is these\\nfixed length descriptors need to be handcrafted and require considerable domain knowledge\\nand expertise. Recently multiple approaches leveraging structure agnostic representations\\nfor material property predictions have been developed.17,23–26In this work, we focus on the\\nRepresentation Learning from Stoichiometry (Roost) framework proposed by Goodall et al.1\\nThe Roost model takes as input the crystal formula and constructs an graph based repre-\\n3',\n",
       " '--- PAGE 3 ---',\n",
       " 'sentation to develop a learnable framework. The Roost architecture is able to predict the\\nmaterial properties with a reasonable accuracy using only the stoichiometric data. In this\\nwork, we utilize the Roost model and propose three different pretraining strategies to improve\\nthe performance of the framework. Our pretraining strategies include 1.) Self Supervised\\nLearning(SSL), 2.) Fingerprint Learning(FL) and 3.) Multimodal Learning(MML). After\\npretraining the Roost model with the 3 pretraining strategies we observe performance gains\\nin multiple material property prediction tasks(Figure 1). The new generative model based\\non diffusion27is called CDVAE.\\nFor our first strategy, we propose the Self-Supervised Learning approach(SSL) for pretrain-\\ning the Roost encoder. In recent years, SSL frameworks28–36have been successfully utilized\\nin computer vision and natural language processing tasks. The successful application of SSL\\nhas spurred many works in molecular machine learning37–39and material science.3,40Draw-\\ning upon the successful strategies of SSL employed in structure-based material property\\nprediction,3,7we propose a framework for structure-agnostic SSL using the Roost encoder\\nfor generating material representation. The core idea of SSL revolves around pretraining\\nmodels without the reliance on explicitly labeled datasets by leveraging the intrinsic infor-\\nmation present in unlabeled data as the training signal. This framework can especially be\\nadvantageous for structure-agnostic material property prediction tasks, where labeled data\\nmay be scarce, and complete structural characterization of material is sometimes unavailable.\\nBy adopting this approach, we address the challenges associated with limited labeled data\\nand inaccessible full structural information. For FL strategy, we devise a simple method-\\nology of predicting the Magpie fingerprint20using the Roost encoder, the core idea is that\\nthe pretrained model can learn the information captured by the fingerprint. Using such a\\nstrategy allows us build a Roost encoder that can retain the benefits of being a learnable\\nframework and also capture information of a fixed descriptor like Magpie fingerprint. We\\nalso introduce a MML strategy in which we leverage the available characterized structure\\ndata and predict the embedding generated using a pretrained CGCNN1encoder from Crys-\\n4',\n",
       " '--- PAGE 4 ---',\n",
       " 'tal Twins Framework.3Using such a strategy we are able to learn the structural information\\nusing our structure agnostic encoder. By incorporating these three strategies, we successfully\\nenhance the performance of the Roost encoder on downstream tasks within the Matbench\\nsuite. Notably, we demonstrate improvements in most material property prediction tasks,\\nhighlighting the effectiveness and potential of our proposed pretraining strategies.\\nLarge Unlabeled DatasetFinetuningRoostEncoderMaterial Property PredictionSrTiO3Small Labeled DatasetPretraining StrategiesFingerprint\\nMultimodal \\nSelf-Supervised SrTiO3SrTiO3xSrTiO3\\nSrTiO3\\nFigure 1: The framework for all the proposed pretraining strategies. We use the Roost\\nencoder to demonstrate the effectiveness of the pretraining strategies for material property\\nprediction tasks. We propose three strategies 1.) Self-Supervised Learning 2.) Fingerprint\\nLearning and 3.) Multimodal Learning. Using these strategies we pretrain the Roost Encoder\\nand finetune the model on different datasets in the Matbench41suite. Using such pretraining\\nstrategies we are able to demonstrate improvements on downstream tasks.\\nReferences\\n(1) Xie, T.; Grossman, J. C. Crystal graph convolutional neural networks for an accurate\\nand interpretable prediction of material properties. Physical review letters 2018 ,120,\\n145301.\\n(2) Karamad, M.; Magar, R.; Shi, Y.; Siahrostami, S.; Gates, I. D.; Farimani, A. B. Orbital\\n5',\n",
       " '--- PAGE 5 ---',\n",
       " 'graph convolutional neural network for material property prediction. Physical Review\\nMaterials 2020 ,4, 093801.\\n(3) Magar, R.; Wang, Y.; Farimani, A. Crystal twins: self-supervised learning for crystalline\\nmaterial property prediction. npj Comput. Mater 2022 ,8, 231.\\n(4) Choudhary, K.; DeCost, B. Atomistic Line Graph Neural Network for improved mate-\\nrials property predictions. npj Computational Materials 2021 ,7, 1–8.\\n(5) Louis, S.-Y.; Zhao, Y.; Nasiri, A.; Wang, X.; Song, Y.; Liu, F.; Hu, J. Graph convolu-\\ntional neural networks with global attention for improved materials property prediction.\\nPhysical Chemistry Chemical Physics 2020 ,22, 18141–18148.\\n(6) Chen, C.; Ye, W.; Zuo, Y.; Zheng, C.; Ong, S. P. Graph networks as a universal\\nmachine learning framework for molecules and crystals. Chemistry of Materials 2019 ,\\n31, 3564–3572.\\n(7) Cao, Z.; Magar, R.; Wang, Y.; Farimani, A. B. MOFormer: Self-Supervised Trans-\\nformer model for Metal-Organic Framework Property Prediction. arXiv preprint\\narXiv:2210.14188 2022 ,\\n(8) Sch¨ utt, K. T.; Sauceda, H. E.; Kindermans, P.-J.; Tkatchenko, A.; M¨ uller, K.-R.\\nSchNet–A deep learning architecture for molecules and materials. The Journal of Chem-\\nical Physics 2018 ,148, 241722.\\n(9) Gasteiger, J.; Groß, J.; G¨ unnemann, S. Directional message passing for molecular\\ngraphs. arXiv preprint arXiv:2003.03123 2020 ,\\n(10) Magar, R.; Farimani, A. B. Learning from mistakes: Sampling strategies to efficiently\\ntrain machine learning models for material property prediction. Computational Mate-\\nrials Science 2023 ,224, 112167.\\n6',\n",
       " '--- PAGE 6 ---',\n",
       " '(11) Xie, T.; Fu, X.; Ganea, O.-E.; Barzilay, R.; Jaakkola, T. S. Crystal Diffusion Variational\\nAutoencoder for Periodic Material Generation. International Conference on Learning\\nRepresentations. 2021.\\n(12) Chen, A.; Zhang, X.; Zhou, Z. Machine learning: accelerating materials development\\nfor energy storage and conversion. InfoMat 2020 ,2, 553–576.\\n(13) Schmidt, J.; Marques, M. R.; Botti, S.; Marques, M. A. Recent advances and applica-\\ntions of machine learning in solid-state materials science. npj Computational Materials\\n2019 ,5, 1–36.\\n(14) Choudhary, K.; DeCost, B.; Chen, C.; Jain, A.; Tavazza, F.; Cohn, R.; Park, C. W.;\\nChoudhary, A.; Agrawal, A.; Billinge, S. J.; Holm, E.; Ong, S. P.; Wolverton, C. Recent\\nadvances and applications of deep learning methods in materials science. npj Compu-\\ntational Materials 2022 ,8, 59.\\n(15) Park, C. W.; Wolverton, C. Developing an improved crystal graph convolutional neu-\\nral network framework for accelerated materials discovery. Physical Review Materials\\n2020 ,4, 063801.\\n(16) Yan, K.; Liu, Y.; Lin, Y.; Ji, S. Periodic Graph Transformers for Crystal Material\\nProperty Prediction. arXiv preprint arXiv:2209.11807 2022 ,\\n(17) Ihalage, A.; Hao, Y. Formula Graph Self-Attention Network for Representation-Domain\\nIndependent Materials Discovery. Advanced Science 2022 , 2200164.\\n(18) Gasteiger, J.; Giri, S.; Margraf, J. T.; G¨ unnemann, S. Fast and uncertainty-aware\\ndirectional message passing for non-equilibrium molecules. 2020 ,\\n(19) Belsky, A.; Hellenbrandt, M.; Karen, V. L.; Luksch, P. New developments in the Inor-\\nganic Crystal Structure Database (ICSD): accessibility in support of materials research\\nand design. Acta Crystallographica Section B: Structural Science 2002 ,58, 364–369.\\n7',\n",
       " '--- PAGE 7 ---',\n",
       " '(20) Ward, L.; Agrawal, A.; Choudhary, A.; Wolverton, C. A general-purpose machine learn-\\ning framework for predicting properties of inorganic materials. npj Computational Ma-\\nterials 2016 ,2, 16028.\\n(21) Botu, V.; Batra, R.; Chapman, J.; Ramprasad, R. Machine learning force fields: con-\\nstruction, validation, and outlook. The Journal of Physical Chemistry C 2017 ,121,\\n511–522.\\n(22) Bart´ ok, A. P.; Kondor, R.; Cs´ anyi, G. On representing chemical environments. Physical\\nReview B 2013 ,87, 184115.\\n(23) Wang, A. Y.-T.; Kauwe, S. K.; Murdock, R. J.; Sparks, T. D. Compositionally restricted\\nattention-based network for materials property predictions. Npj Computational Mate-\\nrials2021 ,7, 77.\\n(24) Chen, C.; Ong, S. P. AtomSets as a hierarchical transfer learning framework for small\\nand large materials datasets. npj Computational Materials 2021 ,7, 173.\\n(25) Goodall, R. E.; Lee, A. A. Predicting materials properties without crystal structure:\\nDeep representation learning from stoichiometry. Nature Communications 2020 ,11,\\n1–9.\\n(26) Goodall, R. E.; Parackal, A. S.; Faber, F. A.; Armiento, R.; Lee, A. A. Rapid discov-\\nery of stable materials by coordinate-free coarse graining. Science Advances 2022 ,8,\\neabn4117.\\n(27) Weng, L. What are diffusion models? lilianweng.github.io 2021 ,\\n(28) Chen, T.; Kornblith, S.; Norouzi, M.; Hinton, G. A simple framework for contrastive\\nlearning of visual representations. International conference on machine learning. 2020;\\npp 1597–1607.\\n8',\n",
       " '--- PAGE 8 ---',\n",
       " '(29) Caron, M.; Misra, I.; Mairal, J.; Goyal, P.; Bojanowski, P.; Joulin, A. Unsuper-\\nvised learning of visual features by contrasting cluster assignments. arXiv preprint\\narXiv:2006.09882 2020 ,\\n(30) Zbontar, J.; Jing, L.; Misra, I.; LeCun, Y.; Deny, S. Barlow twins: Self-supervised learn-\\ning via redundancy reduction. International Conference on Machine Learning. 2021; pp\\n12310–12320.\\n(31) He, K.; Fan, H.; Wu, Y.; Xie, S.; Girshick, R. Momentum contrast for unsupervised\\nvisual representation learning. Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition. 2020; pp 9729–9738.\\n(32) Grill, J.-B.; Strub, F.; Altch´ e, F.; Tallec, C.; Richemond, P. H.; Buchatskaya, E.; Do-\\nersch, C.; Pires, B. A.; Guo, Z. D.; Azar, M. G.; Piot, B.; Kavukcuoglu, K.; Munos, R.;\\nValko, M. Bootstrap your own latent: A new approach to self-supervised learning. arXiv\\npreprint arXiv:2006.07733 2020 ,\\n(33) Chen, X.; He, K. Exploring simple siamese representation learning. Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021; pp 15750–\\n15758.\\n(34) Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; Soricut, R. ALBERT: A\\nLite BERT for Self-supervised Learning of Language Representations. International\\nConference on Learning Representations. 2019.\\n(35) Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. Bert: Pre-training of deep bidi-\\nrectional transformers for language understanding. arXiv preprint arXiv:1810.04805\\n2018 ,\\n(36) Wu, J.; Wang, X.; Wang, W. Y. Self-Supervised Dialogue Learning. Proceedings of\\nthe 57th Annual Meeting of the Association for Computational Linguistics. 2019; pp\\n3857–3867.\\n9',\n",
       " '--- PAGE 9 ---',\n",
       " '(37) Wang, Y.; Magar, R.; Liang, C.; Farimani, A. B. Improving Molecular Contrastive\\nLearning via Faulty Negative Mitigation and Decomposed Fragment Contrast. arXiv\\npreprint arXiv:2202.09346 2022 ,\\n(38) Hu, W.; Liu, B.; Gomes, J.; Zitnik, M.; Liang, P.; Pande, V.; Leskovec, J. Strate-\\ngies For Pre-training Graph Neural Networks. International Conference on Learning\\nRepresentations (ICLR). 2020.\\n(39) Chithrananda, S.; Grand, G.; Ramsundar, B. ChemBERTa: Large-Scale\\nSelf-Supervised Pretraining for Molecular Property Prediction. arXiv preprint\\narXiv:2010.09885 2020 ,\\n(40) Suzuki, Y.; Taniai, T.; Saito, K.; Ushiku, Y.; Ono, K. Self-supervised learning of ma-\\nterials concepts from crystal structures via deep neural networks. Machine Learning:\\nScience and Technology 2022 ,3, 045034.\\n(41) Dunn, A.; Wang, Q.; Ganose, A.; Dopp, D.; Jain, A. Benchmarking materials property\\nprediction methods: the Matbench test set and Automatminer reference algorithm. npj\\nComputational Materials 2020 ,6, 1–10.\\n10',\n",
       " '--- PAGE 10 ---',\n",
       " 'TOC Graphic\\n11']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8c521c-b604-4e33-bc89-0e42905b8448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
